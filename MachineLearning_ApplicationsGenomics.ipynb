{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R46-rwd4hNMn"
   },
   "source": [
    "# Practical session: Machine Learning Applications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of machine learning for the discovery of transcription factor binding sites (TFBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eiiwjw4yhX0P"
   },
   "source": [
    "This tutorial has been developed using the original file and supplement to the manuscript, **A Primer on Deep Learning in Genomics** (*Nature Genetics, 2018*) by James Zou, Mikael Huss, Abubakar Abid, Pejman Mohammadi, Ali Torkamani & Amalio Telentil. [Read the accompanying paper here](https://www.nature.com/articles/s41588-018-0295-5).\n",
    "\n",
    "Find the original tutorial in the Github site [here](https://github.com/abidlabs/deep-learning-genomics-primer/tree/master) or available as a jupyter notebook in Google Colab: <a href=\"https://colab.research.google.com/github/abidlabs/deep-learning-genomics-primer/blob/master/A_Primer_on_Deep_Learning_in_Genomics_Public.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qVh9frJDgVQ-"
   },
   "source": [
    "## Outline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qVh9frJDgVQ-"
   },
   "source": [
    "* **How to Use This Tutorial**\n",
    "* **Introduction to data types**\n",
    "* **0. Background** \n",
    "* **1. Curate the Data**\n",
    "* **2. Select the Architecture and Train**\n",
    "* **3. Evaluate**\n",
    "* **4. Interpret**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Use This Tutorial "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must be already familar with python notebook files. Python notebook combine live code, visualizations, and explanatory text. \n",
    "\n",
    "The notebook is organized into a series of cells. You can modify the Python command and execute each cell as you would a Jupyter notebook. Although it is possible to run all of the cells at once, by choosing **Runtime > Run all** from the menu bar, it is discourage as the practical session it is intented to be execute it chunk by chunk, reading and understanding what is going on. Also, at the end, there are some modifications required for the succesful completion of the practical session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION**: This is an interactive file, in this case, within a computational enviroment that contains the necessary packages to be executed. If you want to succesfully execute this notebook you will need to run it in the UAB linux computers or install several packages such as:\n",
    "- scikit-learn\n",
    "- tensorflow\n",
    "- seaborn\n",
    "- matplotlib\n",
    "- requests\n",
    "- pandas\n",
    "- numpy\n",
    "- biopython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All necessary packages have been previously installed and tested in a computer within a conda environment. The conda configuration has been exported and donwloaded into a file: _deep_learning_env.yaml_, available in github [here](https://raw.githubusercontent.com/JFsanchezherrero/machine_learning_TFBS/refs/heads/main/deep_learning_env.yaml).\n",
    "\n",
    "You can create a new environment and import the same configuration as it was used before. Read additionals details on how to import and existing environment in the original Anaconda Documentation [site](https://docs.anaconda.com/navigator/tutorials/manage-environments/#importing-an-environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the correct environment is activated, load the following libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "B_F7VoAMhLiX",
    "outputId": "ac75f1bd-27c7-4abc-94ed-b81f81ed453f"
   },
   "outputs": [],
   "source": [
    "## load some modules to test the environment works appropriately\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import sklearn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fb-0kBFAts0-"
   },
   "source": [
    "## 0. Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QABdxctitugX"
   },
   "source": [
    "In this practical, we will show how to use deep learning to approach an important problem in functional genomics: **the discovery of transcription-factor binding sites in DNA**.\n",
    "\n",
    "As we go through this notebook, we will  design a [neural network](https://www.geeksforgeeks.org/neural-networks-a-beginners-guide/?ref=gcse) that can discover binding motifs in DNA based on the results of an assay that determines whether a longer DNA sequence binds to the protein or not. Here, the longer **DNA sequences are our *independent variables*** (or *predictors*), while the **positive or negative response of the assay is the *dependent variable* (or *response*)**.\n",
    "\n",
    "We will use simulated data that consists of DNA sequences of length 50 bases (chosen to be artificially short so that the data is easy to play around with), and is labeled with 0 or 1 depending on the result of the assay. Our goal is to build a classifier that can predict whether a particular sequence will bind to the protein and discover the short motif that is the binding site in the sequences that are bound to the protein.\n",
    "\n",
    "(Spoiler alert: the true regulatory motif is *`CGACCGAACTCC`*. Of course, the neural network doesn't know this.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aK7wr8n8gzQ_"
   },
   "source": [
    "## 1. Curate the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the neural network, we must load and preprocess the data, which consists of DNA sequences and their corresponding labels. \n",
    "\n",
    "By processing this data, the network will learn to distinguish sequences that bind to the transcription factor from those that do not. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QRMSFdUSubgX"
   },
   "source": [
    "We will split the data into three different sub-datasets:\n",
    "\n",
    "**(1) Training dataset**: a dataset used to fit the parameters of a model or to define the weights of connections between neurons of a neural network.\n",
    "\n",
    "**(2) Validation dataset**: a second dataset used to minimize overfitting. The weights of the network are not adjusted with this data set. After each training cycle, if the accuracy over the training data set increases, but the accuracy over the validation data set stays the same or decreases, then there is overfitting on the neural network.\n",
    "\n",
    "**(3) Testing dataset**: is a third dataset not included in the training nor validation data sets. After all the training and validation cycles are complete, this dataset is used only for testing the final solution in order to measure the actual predictive power of the neural network on new examples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5T-FgQrNq1vS"
   },
   "source": [
    "![alt text](https://github.com/abidlabs/deep-learning-genomics-primer/blob/master/Screenshot%20from%202018-08-01%2020-31-01.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the simulated data from the original repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCES_URL = 'https://raw.githubusercontent.com/abidlabs/deep-learning-genomics-primer/master/sequences.txt'\n",
    "sequences = requests.get(SEQUENCES_URL).text.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library \"requests\" allow us to connect and download the original txt file hosted in github. \n",
    "\n",
    "See details here: https://requests.readthedocs.io/en/latest/user/quickstart/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the sequences obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## just show the first 5 sequences\n",
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to just print the first few sequences.\n",
    "pd.DataFrame(sequences, index=np.arange(1, len(sequences)+1), \n",
    "             columns=['Sequences']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This removes empty sequences if any\n",
    "sequences = list(filter(None, sequences))  \n",
    "len(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize the data in tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bzsbNHqWiFek"
   },
   "source": [
    "The next  step is to organize the data into a format that can be passed into a deep learning algorithm. Most deep learning algorithms accept data in the form of vectors or matrices (or more generally, [tensors](https://www.kdnuggets.com/2018/05/wtf-tensor.html)). \n",
    "\n",
    "To get each DNA sequence in the form of a matrix, we use **_one-hot encoding_**, which encodes every base in a sequence in the form of a 4-dimensional vector, with a separate dimension for each base. We place a \"1\" in the dimension corresponding to the base found in the DNA sequence, and \"0\"s in all other slots. We then concatenate these 4-dimensional vectors together along the bases in the sequence to form a matrix. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Load and initiate appropriate objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "IPJD6PuDnaS6",
    "outputId": "9d405c43-efa0-4e80-a19d-272986f5ee8e"
   },
   "outputs": [],
   "source": [
    "## 1) Load modules and create objects\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "integer_encoder = LabelEncoder()  ## we just create an object of class LabelEncoder\n",
    "one_hot_encoder = OneHotEncoder(categories='auto')    ## we just create an object of class OneHotEnconder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [LabelEncoder](https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.LabelEncoder.html) object encodes a sequence of bases as a sequence of integers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [OneHotEncoder](https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html) converts an array of integers to a sparse matrix where each row corresponds to one possible value of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a) Hot-one encode example sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cells below, we one-hot encode the simulated DNA sequences, and show an example of what the one-hot encoded sequence looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See and example for ACGT bases\n",
    "my_substring = \"ACGT\"\n",
    "\n",
    "## print separated by \" \" for better visualization\n",
    "print(\"## Original sequence\")\n",
    "print(\" \" + \" \".join(list(my_substring)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit the transformation: \n",
    "print(\"## Fit the transformation using LabelEncoder\")\n",
    "integer_encoded = integer_encoder.fit_transform(list(my_substring))\n",
    "print(integer_encoded)\n",
    "print(type(integer_encoded))\n",
    "print(\"## -- ##\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshape or transpose the transformation: \n",
    "print(\"## Reshape/Transpose\")\n",
    "integer_encoded = np.array(integer_encoded).reshape(-1, 1)\n",
    "print(integer_encoded)\n",
    "print(type(integer_encoded))    \n",
    "print(\"## -- ##\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit the transformation: One-hot encode\n",
    "print(\"## Fit the transformation using OneHotEncoder\")\n",
    "one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "print(one_hot_encoded)\n",
    "print(type(one_hot_encoded))    \n",
    "print(\"## -- ##\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create and stacked array\n",
    "print(\"## Convert to numpy array, stack label alphabetically and transpose\")   \n",
    "print(np.stack(one_hot_encoded.toarray()).T)   \n",
    "print(\"## -- ##\\n\")\n",
    "\n",
    "## just to have it around again\n",
    "print(\"## Original sequence\")\n",
    "print(\" \" + \" \".join(list(my_substring)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, we have **one-hot encode** the simulated DNA sequences, and showed an example of what the one-hot encoded sequence looks like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION**: I will create a function for better reproducibility of the results and to later see the same output with different transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a function with the different visualization of transformations \n",
    "def print_example_transformation(my_substring, bases2show=10):\n",
    "    my_substring = my_substring[:bases2show]\n",
    "    \n",
    "    ## print separated by \" \" for better visualization\n",
    "    print(\"## Original sequence\")\n",
    "    print(\" \" + \" \".join(list(my_substring)))\n",
    "\n",
    "    ## fit the transformation: \n",
    "    print(\"## Fit the transformation using LabelEncoder\")\n",
    "    integer_encoded = integer_encoder.fit_transform(list(my_substring))\n",
    "    print(integer_encoded)\n",
    "    print(type(integer_encoded))\n",
    "    print(\"## -- ##\\n\")\n",
    "    \n",
    "    ## Reshape or transpose the transformation: \n",
    "    print(\"## Reshape\")\n",
    "    integer_encoded = np.array(integer_encoded).reshape(-1, 1)\n",
    "    print(integer_encoded)\n",
    "    print(type(integer_encoded))    \n",
    "    print(\"## -- ##\\n\")\n",
    "    \n",
    "    ## fit the transformation: One-hot encode\n",
    "    print(\"## Fit the transformation using OneHotEncoder\")\n",
    "    one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "    print(one_hot_encoded)\n",
    "    print(type(one_hot_encoded))    \n",
    "    print(\"## -- ##\\n\")\n",
    "\n",
    "    ## Create and stacked array\n",
    "    print(\"## Convert to numpy array, stack label alphabetically and transpose\")   \n",
    "    print(np.stack(one_hot_encoded.toarray()).T)   \n",
    "    print(\"## -- ##\\n\")\n",
    "    \n",
    "    print(\"## Original sequence\")\n",
    "    print(\" \" + \" \".join(list(my_substring)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See and example for ACGT bases\n",
    "my_substring = \"ACGT\"\n",
    "print_example_transformation(my_substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See another example for the first bases of the first sequence:\n",
    "my_substring = sequences[0][:10]\n",
    "print_example_transformation(my_substring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now another example using the argument of the function\n",
    "print_example_transformation(sequences[10], bases2show=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, loop for each sequence and encode the simulated DNA sequence as desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b) Hot-one encode all sequences "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to one-hot encode all sequences available for later processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just load modules and create objects again just in case\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "integer_encoder = LabelEncoder()  \n",
    "one_hot_encoder = OneHotEncoder(categories='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2) one-hot encode the simulated DNA sequence\n",
    "input_features = []\n",
    "for sequence in sequences:\n",
    "    integer_encoded = integer_encoder.fit_transform(list(sequence))\n",
    "    integer_encoded = np.array(integer_encoded).reshape(-1, 1)\n",
    "    one_hot_encoded = one_hot_encoder.fit_transform(integer_encoded)\n",
    "    input_features.append(one_hot_encoded.toarray())\n",
    "\n",
    "## convert to numpy array\n",
    "np.set_printoptions(threshold=40)\n",
    "input_features = np.stack(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show another example of the encoding\n",
    "print(\"Example sequence\\n-----------------------\")\n",
    "print('DNA Sequence #1:\\n',sequences[0][:10],'...',sequences[0][-10:])\n",
    "print('One hot encoding of Sequence #1:\\n',input_features[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## inspect\n",
    "input_features[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _input_features_ is the object containing the information from DNA sequences one-hot encoded to use as input for the neural network. It is stored as tensor (numpy array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Get information for each sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbBmrEVGrahN"
   },
   "source": [
    "Similarly, we can go ahead and load the labels (_response variables_). In this case, the labels are structured as follows: \n",
    "- a \"1\" indicates that a protein bound to the sequence, while\n",
    "- a \"0\" indicates that the protein did not.\n",
    "\n",
    "While we could use the labels as a vector, it is often easier to similarly **one-hot encode the labels**, as we did the features. We carry out that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "IA9FJeQkr1Ze",
    "outputId": "3a4449a0-96ba-4d55-8658-1b6f025b3fd8"
   },
   "outputs": [],
   "source": [
    "## download information from original website\n",
    "LABELS_URL = 'https://raw.githubusercontent.com/abidlabs/deep-learning-genomics-primer/master/labels.txt'\n",
    "\n",
    "labels = requests.get(LABELS_URL).text.split('\\n')\n",
    "labels = list(filter(None, labels))  # removes empty sequences\n",
    "\n",
    "print(labels[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = OneHotEncoder(categories='auto')\n",
    "labels = np.array(labels).reshape(-1, 1)\n",
    "input_labels = one_hot_encoder.fit_transform(labels).toarray()\n",
    "\n",
    "print('Labels:\\n',labels.T)\n",
    "print('One-hot encoded labels:\\n',input_labels.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MTokFzDZvQR-"
   },
   "source": [
    "We also go ahead and split the data into training and test sets. The purpose of the test set is to ensure that we can observe the performance of the model on new data, not seen previously during training. \n",
    "\n",
    "At a later step, we will further partition the training set into a training and validation set.\n",
    "\n",
    "We will be using function [train_test_split](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.train_test_split.html) that quickly splits arrays or matrices into random train and test subsets.\n",
    "\r\n",
    "\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Provided parameters are:\n",
    "- **Arrays**: Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes. We provide **input_features** and **input_labels**\n",
    "- **test_size**: Value between 0.0 and 1.0 that represents the proportion of the dataset to include in the test split.\n",
    "- **random_state**: Controls the shuffling applied to the data before applying the split. Pass an int for reproducible output across multiple function calls\n",
    "\n",
    "Return values are list containing train-test split of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_7LKgvc3Lnn"
   },
   "outputs": [],
   "source": [
    "## load package and split the input information: sequences and labels, as tensors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    input_features, input_labels, test_size=0.25, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION**: Create a simple function to check multiple parameters of tensors created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_arrays(array_given):\n",
    "    print(\"## Type:\")\n",
    "    print(type(array_given))\n",
    "    \n",
    "    print(\"## Number dimensions:\")\n",
    "    print(array_given.ndim)\n",
    "    \n",
    "    print(\"## Shape:\")\n",
    "    print(array_given.shape)\n",
    "\n",
    "    print(\"## Size:\")\n",
    "    print(array_given.size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_arrays(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_arrays(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_arrays(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_arrays(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7LQp2ZFrg6dm"
   },
   "source": [
    "## 2. Select the Architecture and Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xBT6Q3j-sjhh"
   },
   "source": [
    "![alt text](https://github.com/abidlabs/deep-learning-genomics-primer/blob/master/Screenshot%20from%202018-08-01%2020-31-49.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krHJgtK_rzif"
   },
   "source": [
    "Next, we choose a neural network architecture to [train the model](https://www.geeksforgeeks.org/implementing-neural-networks-using-tensorflow/). In this tutorial, we choose a simple 1D convolutional neural network (CNN), which is commonly used in deep learning for functional genomics applications.\n",
    "\n",
    "A CNN learns to recognize patterns that are generally invariant across space, by trying to match the input sequence to a number of learnable \"filters\" of a fixed size. In our dataset, the filters will be motifs within the DNA sequences. The CNN may then learn to combine these filters to recognize a larger structure (e.g. the presence or absence of a transcription factor binding site). \n",
    "\n",
    "We will use the deep learning library `Keras`. As of 2017, `Keras` has been integrated into `TensorFlow`,  which makes it very easy to construct neural networks. We only need to specify the kinds of layers we would like to include in our network, and the dimensionality of each layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you can try using the linear model, since the neural network basically follows the same ‘math’ as regression you can create a linear model using a neural network as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "dU3imaIns80_",
    "outputId": "851993e7-b2f6-43fd-f061-65e5abd7d76a"
   },
   "outputs": [],
   "source": [
    "## Load modules\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = Sequential()\n",
    "linear_model.add(Dense(units=1, input_shape=[train_features.shape[1]]))\n",
    "\n",
    "linear_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is basically a linear model, what if your dataset is a bit more complex, and the relations between the features are much more diverse and you want a non-linear model? What do you need? The answer is [Activation Functions](https://www.geeksforgeeks.org/activation-functions-neural-networks/). This is where neural networks truly start to shine. \n",
    "\n",
    "We will not go in-depth about activation functions here but basically, these add/introduce non-linearity to our model, the more you use them the more complex patterns our model can find. An activation function determines the output of a neuron in a neural network by adding non-linearity, enabling the network to learn complex patterns from the data.\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN we generate in this example consists of the following layers:\n",
    "\n",
    "- _Conv1D_: We define our convolutional layer to have 32 filters of size 12 bases.\n",
    "\n",
    "- _MaxPooling1D_: After the convolution, we use a pooling layer to down-sample the output of the each of the 32 convolutional filters. Though not always required, this is a typical form of non-linear down-sampling used in CNNs.\n",
    "\n",
    "- _Flatten_: This layer flattens the output of the max pooling layer, combining the results of the convolution and pooling layers across all 32 filters. \n",
    "\n",
    "- _Dense_: The first Dense tensor creates a layer (dense_1) that compresses the representation of the flattened layer, resulting in smaller layer with 16 tensors, and the second Dense function converges the tensors into the output layer (dense_2) that consists of the two possible response values (0 or 1).\n",
    "\n",
    "We can see the details of the architecture of the neural network we have created by running `model.summary()`, which prints the dimensionality and number of parameters for each layer in our network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN = Sequential()\n",
    "model_CNN.add(Conv1D(filters=32, kernel_size=12, \n",
    "                 input_shape=(train_features.shape[1], 4)))\n",
    "model_CNN.add(MaxPooling1D(pool_size=4))\n",
    "model_CNN.add(Flatten())\n",
    "model_CNN.add(Dense(16, activation='relu')) ## relu (rectified linear unit) \n",
    "model_CNN.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_features.shape[1], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model_CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Keras after you create your model, you need to ‘compile’ other parameters for it, like it’s shown below. This is kind of like us setting all the parameters for our model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam optimizer works pretty well for\n",
    "# all kinds of problems and is a good starting point\n",
    "model_CNN.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "## as metrics we required the binary_accuraccy as we are have a categorical variable.\n",
    "## We might use the mean absolute error if we have a continuous variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_CNN.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qSOUwoG_vHRA"
   },
   "source": [
    "Now, we are ready to go ahead and train the neural network. We will further divide the training set into a training and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "colab_type": "code",
    "id": "LSOmHIM83hXO",
    "outputId": "1bfe5b1d-73ad-4c18-e343-7bcd51b6d3fc"
   },
   "outputs": [],
   "source": [
    "history_CNN = model_CNN.fit(train_features, train_labels, \n",
    "                    epochs=50, verbose=0, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find additional information for the model.fit function in the original website [here](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train only on the reduced training set, but plot the loss curve on both the training and validation sets. Once the loss for the validation set stops improving or gets worse throughout the learning cycles, it is time to stop training because the model has already converged and may be just overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the keras model generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_CNN.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(history_CNN.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_CNN.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the data stored within the history.history object. Show it as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(history_CNN.history).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets find out about the performance of the model by plotting the accuracy and loss curve which are two common tools we use to understand how well a machine learning model is learning and getting better over time. In the simplest terms, they help us evaluate the model's performance during training. \n",
    "\n",
    "On the one hand, the accuracy curve records how accurate the model’s predictions are on the given data, while the loss curve records the actual difference between the model’s prediction and the actual true output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOSS Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss curve, or training loss curve, gives us insights into how the model's performance improves over time by measuring the error (or dissimilarity) between its predicted output and the true output. The loss represents how far off the model's predictions are from the actual values. \n",
    "\n",
    "By minimizing the loss, the model aims to make its predictions as close as possible to the true values. \n",
    "Put simply: the loss curve shows us how the model's error decreases as it learns, which indicates an improvement in its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "colab_type": "code",
    "id": "LSOmHIM83hXO",
    "outputId": "1bfe5b1d-73ad-4c18-e343-7bcd51b6d3fc"
   },
   "outputs": [],
   "source": [
    "## plot the results training as LOSS curve\n",
    "plt.figure()\n",
    "plt.plot(history_CNN.history['loss'])\n",
    "plt.plot(history_CNN.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5eKDmX8ODBE"
   },
   "source": [
    "Similarly, we can plot the accuracy of our neural network on the binary classification task. The metric used in this example is the _binary accuracy_, which calculates the proportion of predictions that match labels or response variables. Other metrics may be used in different tasks -- for example, the _mean squared error_ is typically used to measure the accuracy for continuous response variables (e.g. polygenic risk scores, total serum cholesterol level, height, weight and systolic blood pressure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "J2Jdpa1i8zqM",
    "outputId": "a1bfe431-0bd4-4c51-d283-dbe6fefd13f7"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history_CNN.history['binary_accuracy'])\n",
    "plt.plot(history_CNN.history['val_binary_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading [here](https://wandb.ai/mostafaibrahim17/ml-articles/reports/A-Deep-Dive-Into-Learning-Curves-in-Machine-Learning--Vmlldzo0NjA1ODY0 ) the guide on accuracy and loss curves for a better understanding of the interpretation of this plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Xy7VhhZg-hN"
   },
   "source": [
    "## 3. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "btf7FyMVsnFA"
   },
   "source": [
    "![alt text](https://github.com/abidlabs/deep-learning-genomics-primer/blob/master/Screenshot%20from%202018-08-01%2020-32-12.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQ_xYCvfvFlE"
   },
   "source": [
    "The best way to evaluate whether the network has learned to classify sequences is to evaluate its performance on a fresh test set consisting of data that it has not observed at all during training. Here, we evaluate the model on the test set and plot the results as a confusion matrix. Nearly every test sequence should be correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "colab_type": "code",
    "id": "J1bvxV9J-EMT",
    "outputId": "43bd6cbf-671f-4701-b8b2-569a3871d394"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "predicted_labels = model_CNN.predict(np.stack(test_features))\n",
    "cm = confusion_matrix(np.argmax(test_labels, axis=1), \n",
    "                      np.argmax(predicted_labels, axis=1))\n",
    "print('Confusion matrix:\\n',cm)\n",
    "\n",
    "cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "\n",
    "plt.imshow(cm, cmap=plt.cm.Blues)\n",
    "plt.title('Normalized confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('True label')\n",
    "plt.ylabel('Predicted label')\n",
    "plt.xticks([0, 1]); plt.yticks([0, 1])\n",
    "plt.grid('off')\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], '.2f'),\n",
    "             horizontalalignment='center',\n",
    "             color='white' if cm[i, j] > 0.5 else 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the test data and compute the accuracy and loss of the predicted and actual presence of transcription binding sites in each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model_CNN.evaluate(train_features, train_labels)\n",
    "print('Loss = ',loss_and_metrics[0])\n",
    "print('Accuracy = ',loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the performance by understanding and tuning the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By tuning different parameters in simple machine learning models we can get to understand how it works and how to unlock the full potential of it.\n",
    "\n",
    "I will create several functions for the reproducibility and usability of the code, show how to use them and finally, ask some questions to address your research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "## create CNN model\n",
    "#######################################\n",
    "def get_cnn_model(train_f, option=1):\n",
    "    if option==1:\n",
    "        ## create the CNN model use before\n",
    "        model2return = Sequential()\n",
    "        model2return.add(Conv1D(filters=32, kernel_size=12, \n",
    "                         input_shape=(train_f.shape[1], 4)))\n",
    "        model2return.add(MaxPooling1D(pool_size=4))\n",
    "        model2return.add(Flatten())\n",
    "        model2return.add(Dense(16, activation='relu')) ## relu (rectified linear unit) \n",
    "        model2return.add(Dense(2, activation='softmax'))\n",
    "    else:\n",
    "        ## add another model\n",
    "        model2return = \"\"\n",
    "\n",
    "    ## compile model\n",
    "    model2return.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "              metrics=['binary_accuracy'])\n",
    "\n",
    "    return model2return\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test function to produce the CNN model of study or other options\n",
    "test_model = get_cnn_model(train_f=train_features, option=1)\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "## create plots model\n",
    "#######################################\n",
    "def create_plots(history2use_here, cm_given):\n",
    "    \n",
    "    # Initialise the subplot function using number of rows and columns\n",
    "    figure, axis = plt.subplots(1, 3, figsize=(12, 6))    \n",
    "    \n",
    "    ## plot the results training as LOSS curve\n",
    "    axis[0].plot(history2use_here.history['loss'])\n",
    "    axis[0].plot(history2use_here.history['val_loss'])\n",
    "    axis[0].set_title('model loss')\n",
    "    axis[0].set_ylabel('loss')\n",
    "    axis[0].set_xlabel('epoch')\n",
    "    axis[0].legend(['train', 'validation'])\n",
    "    \n",
    "    ## plot accuracy curve\n",
    "    axis[1].plot(history2use_here.history['binary_accuracy'])\n",
    "    axis[1].plot(history2use_here.history['val_binary_accuracy'])\n",
    "    axis[1].set_title('model accuracy')\n",
    "    axis[1].set_ylabel('accuracy')\n",
    "    axis[1].set_xlabel('epoch')\n",
    "    axis[1].legend(['train', 'validation'])\n",
    "    \n",
    "    #figure.legend(['train', 'validation'], loc='outside right upper')\n",
    "\n",
    "    ## plot the confusion matrix for True/False observed/predicted\n",
    "    axis[2].imshow(cm_given, cmap=plt.cm.Blues)\n",
    "    axis[2].set_title('Normalized confusion matrix')\n",
    "    axis[2].set_xlabel('True label')\n",
    "    axis[2].set_ylabel('Predicted label')\n",
    "    axis[2].set_xticks([0, 1]); \n",
    "    axis[2].set_yticks([0, 1])\n",
    "    axis[2].grid('off')\n",
    "    for i, j in itertools.product(range(cm_given.shape[0]), range(cm_given.shape[1])):\n",
    "        axis[2].text(j, i, format(cm_given[i, j], '.2f'),\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm_given[i, j] > 0.5 else 'black')\n",
    "\n",
    "    \n",
    "    # Combine all the operations and display\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test example function to plot at once the three plots of interest to evaluate the performance\n",
    "create_plots(history_CNN, cm_given=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "## create a function to reproduce the model many times\n",
    "#######################################\n",
    "def example_CNN(features2used, labels2use, \n",
    "                random_state_int=123, test_size_float=0.25, model_option=1,\n",
    "                epochs2use=50, verb_2use=0, val_2use=0.25): ## set defaults\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    \n",
    "    ## split the dataset\n",
    "    train_features2use, test_features2use, train_labels2use, test_labels2use = train_test_split(\n",
    "        features2used, labels2use, test_size=test_size_float, random_state=random_state_int)\n",
    "\n",
    "    if verb_2use>0:\n",
    "        check_arrays(train_features2use)\n",
    "        check_arrays(test_features2use)\n",
    "        check_arrays(train_labels2use)\n",
    "        check_arrays(test_labels2use)\n",
    "    \n",
    "    ## create the model\n",
    "    model2use = get_cnn_model(train_f=train_features2use, option=model_option)\n",
    "\n",
    "    if verb_2use>0:\n",
    "        print(model2use.summary())\n",
    "        print(model2use)\n",
    "\n",
    "    ## train\n",
    "    history2use = model2use.fit(train_features2use, train_labels2use, \n",
    "                    epochs=epochs2use, verbose=verb_2use, validation_split=val_2use)\n",
    "    \n",
    "    \n",
    "    ## evaluate\n",
    "    predicted_labels2use = model2use.predict(np.stack(test_features2use))\n",
    "    cm2use = confusion_matrix(np.argmax(test_labels2use, axis=1), \n",
    "                          np.argmax(predicted_labels2use, axis=1))\n",
    "    cm2use = cm2use.astype('float') / cm2use.sum(axis = 1)[:, np.newaxis]\n",
    "\n",
    "    print('Confusion matrix:\\n',cm2use)\n",
    "\n",
    "    ## create plots\n",
    "    create_plots(history2use, cm2use)\n",
    "\n",
    "    loss_and_metrics2use = model2use.evaluate(train_features2use, train_labels2use)\n",
    "    if verb_2use>0:\n",
    "        print('Loss = ',loss_and_metrics2use[0])\n",
    "        print('Accuracy = ',loss_and_metrics2use[1])\n",
    "\n",
    "    return loss_and_metrics2use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## usage example\n",
    "example_CNN(features2used=input_features, labels2use=input_labels, epochs2use=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions to address:\n",
    "\n",
    "- Find what is the effect of the verbose parameter in the _model.fit()_ [or supplied as _verb_2use_ the _example_CNN()_]\n",
    "- Test the effect of applying a more simple model [Hint: reduce layers of the model]\n",
    "- Test the effect of epoch in the prediction: 2, 5, 10, 25, 50, 100, 1000\n",
    "- Test the effect of applying a different test/train set partition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UBdJQC1Ug__3"
   },
   "source": [
    "## 5. Interpret the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6qmKi1ResqHo"
   },
   "source": [
    "![alt text](https://github.com/abidlabs/deep-learning-genomics-primer/blob/master/Screenshot%20from%202018-08-01%2020-32-31.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UpAwoK9SwAbb"
   },
   "source": [
    "Your results so far should allow you to conclude that the neural network is quite effective in learning to distinguish sequences that bind the protein from sequences that do not. But can we understand _why_ the neural network classifies a training point in the way that it does? To do so, we can compute a simple _saliency map_, which is the gradient of the model's prediction with respect to each individual nucleotide. \n",
    "\n",
    "In other words, the saliency maps shows how the output response value changes with respect to a small changes in input nucleotide sequence. All the positive values in the gradients tell us that a small change to that nucleotide will change the output value. Hence, visualizing these gradients for a given input sequence, should provide some clues about what nucleotides form the binding motive that we are trying to identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def compute_salient_bases(model, x):\n",
    "    @tf.function\n",
    "    def compute_gradients(x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(x)\n",
    "            logits = model(x)\n",
    "            prob = logits[:, 1]  # Assuming binary classification, change the index if needed\n",
    "        gradients = tape.gradient(prob, x)\n",
    "        return gradients\n",
    "\n",
    "    x_value = np.expand_dims(x, axis=0)\n",
    "    gradients = compute_gradients(x_value)\n",
    "    gradients = tf.where(gradients == None, tf.zeros_like(gradients), gradients)\n",
    "    sal = tf.reduce_sum(gradients * x, axis=2)\n",
    "    sal = tf.clip_by_value(sal, clip_value_min=0, clip_value_max=tf.reduce_max(sal))\n",
    "    return sal.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Positive sequence\n",
    "sequence_index = 1999  \n",
    "sal = compute_salient_bases(model_CNN, input_features[sequence_index])\n",
    "\n",
    "plt.figure(figsize=[16,5])\n",
    "barlist = plt.bar(np.arange(len(sal[0])), sal[0])\n",
    "[barlist[i].set_color('C1') for i in range(5,17)]  # Change the coloring here if you change the sequence index.\n",
    "plt.xlabel('Bases')\n",
    "plt.ylabel('Magnitude of saliency values')\n",
    "plt.xticks(np.arange(len(sal[0])), list(sequences[sequence_index]));\n",
    "plt.title('Saliency map for bases in one of the positive sequences'\n",
    "          ' (orange indicates the actual bases in motif)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative sequence\n",
    "sequence_index = 0  \n",
    "sal = compute_salient_bases(model_CNN, input_features[sequence_index])\n",
    "\n",
    "plt.figure(figsize=[16,5])\n",
    "barlist = plt.bar(np.arange(len(sal[0])), sal[0])\n",
    "[barlist[i].set_color('C1') for i in range(5,17)]  # Change the coloring here if you change the sequence index.\n",
    "plt.xlabel('Bases')\n",
    "plt.ylabel('Magnitude of saliency values')\n",
    "plt.xticks(np.arange(len(sal[0])), list(sequences[sequence_index]));\n",
    "plt.title('Saliency map for bases in one of the negative sequences'\n",
    "          ' (orange indicates the actual bases in motif)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "voPIrz9TPtIH"
   },
   "source": [
    "The results above should show high saliency values for the bases _CGACCGAACTCC_ appearing in the DNA sequence. If you recall from the top of the document, this is exactly the motif that we embedded in the positive sequences! The raw saliency values may be non-zero for other bases as well -- the gradient-based saliency map method is not perfect, and there other more complex interpretation methods that are used in practice to obtain better results.  \n",
    "\n",
    "Furthermore, we may explore other architectures for our neural network to see if we can improve performance on the validation dataset. For example, we could choose different _hyper-parameters_, which are variables that define the network structure (e.g. the number of dense or convolutional layers, the dimensionality of each layer, etc.) and variables that determine how the network is trained (e.g. the number of epochs, the learning rate, etc.). Testing different hyper-parameter values or performing a hyper-parameter search grid are good practices that may help the deep learning procedure to obtain a clearer signal for classifying sequences and identifying the binding motif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1lGpn-5IZ3VN"
   },
   "source": [
    "## Acknowledgements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVFi5aDBZ7M8"
   },
   "source": [
    "Thanks to Julia di lulio and Raquel Dias for helpful comments and suggestions in preparing this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and examples\n",
    "- https://www.kdnuggets.com/2018/05/wtf-tensor.html\n",
    "- https://www.geeksforgeeks.org/implementing-neural-networks-using-tensorflow/ \n",
    "- https://www.geeksforgeeks.org/neural-networks-a-beginners-guide/?ref=gcse\n",
    "- https://wandb.ai/mostafaibrahim17/ml-articles/reports/A-Deep-Dive-Into-Learning-Curves-in-Machine-Learning--Vmlldzo0NjA1ODY0\n",
    "- https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/\n",
    "- https://www.bmc.com/blogs/keras-neural-network-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vY3uDUtcLwXe"
   },
   "source": [
    "# GitHub Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U8soCi3qLzmc"
   },
   "source": [
    "If you found this tutorial helpful, kindly star the [associated GitHub repo](https://github.com/abidlabs/deep-learning-genomics-primer/blob/master/A_Primer_on_Deep_Learning_in_Genomics_Public.ipynb) so that it is more visible to others as well!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "A Primer on Deep Learning in Genomics - Public.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
